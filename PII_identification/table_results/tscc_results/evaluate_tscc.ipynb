{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook shows results from Table 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_metrics(gold_csv, pred_csv):\n",
    "    \"\"\"\n",
    "    Computes overall TP, FP, FN, Precision, Recall, F1 score, and F5 score \n",
    "    for name detection by comparing two CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - gold_csv: Path to the gold standard CSV file.\n",
    "    - pred_csv: Path to the predicted annotations CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with overall performance metrics.\n",
    "    \"\"\"\n",
    "    # Read CSV files\n",
    "    gold_df = pd.read_csv(gold_csv)\n",
    "    pred_df = pd.read_csv(pred_csv)\n",
    "\n",
    "    # Group by 'file_idx' and 'entity_text' and count occurrences\n",
    "    gold_counts = gold_df.groupby(['file_idx', 'entity_text']).size().reset_index(name='gold_count')\n",
    "    pred_counts = pred_df.groupby(['file_idx', 'entity_text']).size().reset_index(name='pred_count')\n",
    "\n",
    "    # Merge counts DataFrames\n",
    "    merged = pd.merge(gold_counts, pred_counts, on=['file_idx', 'entity_text'], how='outer').fillna(0)\n",
    "\n",
    "    # Ensure counts are integers\n",
    "    merged['gold_count'] = merged['gold_count'].astype(int)\n",
    "    merged['pred_count'] = merged['pred_count'].astype(int)\n",
    "\n",
    "    # Compute TP, FP, FN for each entity\n",
    "    merged['TP'] = merged.apply(lambda row: min(row['gold_count'], row['pred_count']), axis=1)\n",
    "    merged['FP'] = merged['pred_count'] - merged['TP']\n",
    "    merged['FN'] = merged['gold_count'] - merged['TP']\n",
    "\n",
    "    # Sum up over all entities\n",
    "    TP = merged['TP'].sum()\n",
    "    FP = merged['FP'].sum()\n",
    "    FN = merged['FN'].sum()\n",
    "\n",
    "    # Calculate precision, recall, F1 score, and F5 score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    F1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    F5 = (1 + 5**2) * (precision * recall) / ((5**2 * precision) + recall) if ((5**2 * precision) + recall) > 0 else 0\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'TP': int(TP),\n",
    "        'FP': int(FP),\n",
    "        'FN': int(FN),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'F1 Score': round(F1, 4),\n",
    "        'F5 Score': round(F5, 4)\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Presidio (en_core_web_trf):\n",
      "{'TP': 1513, 'FP': 2694, 'FN': 102, 'Precision': 0.3596, 'Recall': 0.9368, 'F1 Score': 0.5198, 'F5 Score': 0.8824}\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "2. Azure AI Language:\n",
      "{'TP': 1320, 'FP': 1316, 'FN': 295, 'Precision': 0.5008, 'Recall': 0.8173, 'F1 Score': 0.621, 'F5 Score': 0.7979}\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "3. GPT-4o-mini + Few-shot Prompting (3 shots):\n",
      "{'TP': 1604, 'FP': 641, 'FN': 11, 'Precision': 0.7145, 'Recall': 0.9932, 'F1 Score': 0.8311, 'F5 Score': 0.9785}\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "4. Fine-tuned GPT-4o-mini + Zero-shot Prompting:\n",
      "{'TP': 1273, 'FP': 2, 'FN': 342, 'Precision': 0.9984, 'Recall': 0.7882, 'F1 Score': 0.881, 'F5 Score': 0.7947}\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "5. GPT-4o-mini + Fine-tuning (on 10 TSCC transcripts):\n",
      "{'TP': 1561, 'FP': 26, 'FN': 54, 'Precision': 0.9836, 'Recall': 0.9666, 'F1 Score': 0.975, 'F5 Score': 0.9672}\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "6. Fine-tuned GPT-4o-mini + Fine-tuning (on 10 TSCC transcripts):\n",
      "{'TP': 1598, 'FP': 48, 'FN': 17, 'Precision': 0.9708, 'Recall': 0.9895, 'F1 Score': 0.9801, 'F5 Score': 0.9887}\n",
      "\n",
      "========================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the true data path and model outputs\n",
    "true_file_path = 'data/new_labels_test.csv'\n",
    "models_to_evaluate = {\n",
    "    \"1. Presidio (en_core_web_trf)\": \"output/1_presidio.csv\",\n",
    "    \"2. Azure AI Language\": \"output/2_azure.csv\",\n",
    "    \"3. GPT-4o-mini + Few-shot Prompting (3 shots)\": \"output/3_gpt+fewshot.csv\",\n",
    "    \"4. Fine-tuned GPT-4o-mini + Zero-shot Prompting\": \"output/4_ftgpt+zeroshot.csv\",\n",
    "    \"5. GPT-4o-mini + Fine-tuning (on 10 TSCC transcripts)\": \"output/5_gpt+ft.csv\",\n",
    "    \"6. Fine-tuned GPT-4o-mini + Fine-tuning (on 10 TSCC transcripts)\": \"output/6_ftgpt+ft.csv\"\n",
    "}\n",
    "\n",
    "# Evaluate and print metrics\n",
    "for model_name, detected_file_path in models_to_evaluate.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    metrics = calculate_metrics(true_file_path, detected_file_path)\n",
    "    print(metrics)\n",
    "    print(\"\\n\" + \"=\"*120 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
