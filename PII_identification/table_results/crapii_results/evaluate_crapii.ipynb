{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56982560",
   "metadata": {},
   "source": [
    "#### This notebook shows results from Table 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40f38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_metrics(gold_csv, pred_csv):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, F1 score, and F5 score for each type and overall between two CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - gold_csv: Path to the gold standard CSV file.\n",
    "    - pred_csv: Path to the predicted annotations CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame with precision, recall, F1 score, and F5 score for each type and overall.\n",
    "    \"\"\"\n",
    "    # Read CSV files\n",
    "    gold_df = pd.read_csv(gold_csv)\n",
    "    pred_df = pd.read_csv(pred_csv)\n",
    "\n",
    "    # Mapping of Presidio-detected entity types to true entity types\n",
    "    entity_type_mapping_pre = {\n",
    "        \"PERSON\": \"NAME_STUDENT\",\n",
    "        \"EMAIL_ADDRESS\": \"EMAIL\",\n",
    "        \"URL\": \"URL_PERSONAL\",\n",
    "        \"PHONE_NUMBER\": \"PHONE_NUM\"\n",
    "    }\n",
    "\n",
    "    # Mapping of Azure-detected entity types to true entity types\n",
    "    entity_type_mapping_az = {\n",
    "        \"Person\": \"NAME_STUDENT\",\n",
    "        \"Email\": \"EMAIL\",\n",
    "        \"URL\": \"URL_PERSONAL\",\n",
    "        \"PhoneNumber\": \"PHONE_NUM\"\n",
    "    }\n",
    "\n",
    "    # Apply the mapping to standardize the entity types in pred_df\n",
    "    if 'azure' in pred_csv:\n",
    "        pred_df['type'] = pred_df['type'].map(entity_type_mapping_az).fillna(pred_df['type'])\n",
    "    else:\n",
    "        pred_df['type'] = pred_df['type'].map(entity_type_mapping_pre).fillna(pred_df['type'])\n",
    "\n",
    "    # Define the types\n",
    "    types = ['NAME_STUDENT', 'URL_PERSONAL', 'EMAIL', 'PHONE_NUM']\n",
    "\n",
    "    # Initialize counts for each type and overall\n",
    "    counts = {}\n",
    "    for entity_type in types + ['Overall']:\n",
    "        counts[entity_type] = {'TP': 0, 'FP': 0, 'FN': 0}\n",
    "\n",
    "    # Compute counts for each type\n",
    "    for entity_type in types:\n",
    "        # Filter entities by type\n",
    "        gold_entities = gold_df[gold_df['type'] == entity_type]\n",
    "        pred_entities = pred_df[pred_df['type'] == entity_type]\n",
    "\n",
    "        # Initialize matched indices\n",
    "        matched_gold_indices = set()\n",
    "        \n",
    "        # Group by file_idx for efficient matching\n",
    "        gold_grouped = gold_entities.groupby('file_idx')\n",
    "        pred_grouped = pred_entities.groupby('file_idx')\n",
    "\n",
    "        # Get all file indices present in either gold or pred\n",
    "        all_file_indices = set(gold_entities['file_idx']).union(set(pred_entities['file_idx']))\n",
    "\n",
    "        for file_idx in all_file_indices:\n",
    "            gold_file_entities = gold_grouped.get_group(file_idx) if file_idx in gold_grouped.groups else pd.DataFrame()\n",
    "            pred_file_entities = pred_grouped.get_group(file_idx) if file_idx in pred_grouped.groups else pd.DataFrame()\n",
    "\n",
    "            # Reset indices to ensure unique identification within this loop\n",
    "            gold_file_entities = gold_file_entities.reset_index()\n",
    "            pred_file_entities = pred_file_entities.reset_index()\n",
    "\n",
    "            # Create lists of gold and pred entities for this file_idx\n",
    "            gold_list = gold_file_entities.to_dict('records')\n",
    "            pred_list = pred_file_entities.to_dict('records')\n",
    "\n",
    "            # Match predicted entities to gold entities\n",
    "            for pred_entity in pred_list:\n",
    "                pred_text = pred_entity['entity_text']\n",
    "                pred_type = pred_entity['type']\n",
    "                \n",
    "                for gold_entity in gold_list:\n",
    "                    gold_text = gold_entity['entity_text']\n",
    "                    gold_type = gold_entity['type']\n",
    "                    \n",
    "                    # Use the correct global unique identifier from the original 'index' column\n",
    "                    gold_global_idx = gold_entity['index'] \n",
    "\n",
    "                    # Check if this gold entity has already been matched\n",
    "                    if gold_global_idx in matched_gold_indices:\n",
    "                        continue\n",
    "\n",
    "                    # Check for matching criteria\n",
    "                    if (pred_text == gold_text and pred_type == gold_type):\n",
    "                        # Match found\n",
    "                        matched_gold_indices.add(gold_global_idx)\n",
    "                        break  # Move to next predicted entity\n",
    "\n",
    "        # Compute counts\n",
    "        TP = len(matched_gold_indices)\n",
    "        FP = len(pred_entities) - TP\n",
    "        FN = len(gold_entities) - TP\n",
    "\n",
    "        # Update counts\n",
    "        counts[entity_type]['TP'] = TP\n",
    "        counts[entity_type]['FP'] = FP\n",
    "        counts[entity_type]['FN'] = FN\n",
    "\n",
    "    # Compute overall counts\n",
    "    for metric in ['TP', 'FP', 'FN']:\n",
    "        counts['Overall'][metric] = sum(counts[entity_type][metric] for entity_type in types)\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for entity_type in types + ['Overall']:\n",
    "        TP = counts[entity_type]['TP']\n",
    "        FP = counts[entity_type]['FP']\n",
    "        FN = counts[entity_type]['FN']\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        F1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        F5 = (1 + 5**2) * (precision * recall) / ((5**2 * precision) + recall) if ((5**2 * precision) + recall) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            'Entity Type': entity_type,\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'FN': FN,\n",
    "            'Precision': round(precision, 4),\n",
    "            'Recall': round(recall, 4),\n",
    "            'F1 Score': round(F1, 4),\n",
    "            'F5 Score': round(F5, 4)\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc343798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Presidio (en_core_web_lg):\n",
      "    Entity Type    TP     FP   FN  Precision  Recall  F1 Score  F5 Score\n",
      "0  NAME_STUDENT  1805   9294  805     0.1626  0.6916    0.2633    0.6147\n",
      "1  URL_PERSONAL   181   2256   31     0.0743  0.8538    0.1367    0.6082\n",
      "2         EMAIL    61     10    1     0.8592  0.9839    0.9173    0.9784\n",
      "3     PHONE_NUM     8     37    1     0.1778  0.8889    0.2963    0.7704\n",
      "4       Overall  2055  11597  838     0.1505  0.7103    0.2484    0.6214\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2. Presidio (en_core_web_trf):\n",
      "    Entity Type    TP    FP   FN  Precision  Recall  F1 Score  F5 Score\n",
      "0  NAME_STUDENT  2172  6849  438     0.2408  0.8322    0.3735    0.7604\n",
      "1  URL_PERSONAL   180  2257   32     0.0739  0.8491    0.1359    0.6049\n",
      "2         EMAIL    61    10    1     0.8592  0.9839    0.9173    0.9784\n",
      "3     PHONE_NUM     8    37    1     0.1778  0.8889    0.2963    0.7704\n",
      "4       Overall  2421  9153  472     0.2092  0.8368    0.3347    0.7503\n",
      "\n",
      "================================================================================\n",
      "\n",
      "3. Azure AI Language:\n",
      "    Entity Type    TP    FP   FN  Precision  Recall  F1 Score  F5 Score\n",
      "0  NAME_STUDENT  2451  7074  159     0.2573  0.9391    0.4040    0.8522\n",
      "1  URL_PERSONAL   145   917   67     0.1365  0.6840    0.2276    0.5926\n",
      "2         EMAIL    61     8    1     0.8841  0.9839    0.9313    0.9796\n",
      "3     PHONE_NUM     8   161    1     0.0473  0.8889    0.0899    0.5279\n",
      "4       Overall  2665  8160  228     0.2462  0.9212    0.3885    0.8333\n",
      "\n",
      "================================================================================\n",
      "\n",
      "4. Prompting GPT-4o-mini:\n",
      "    Entity Type    TP    FP   FN  Precision  Recall  F1 Score  F5 Score\n",
      "0  NAME_STUDENT  2036   750  574     0.7308  0.7801    0.7546    0.7781\n",
      "1  URL_PERSONAL   153   313   59     0.3283  0.7217    0.4513    0.6899\n",
      "2         EMAIL    57    55    5     0.5089  0.9194    0.6552    0.8917\n",
      "3     PHONE_NUM     5    45    4     0.1000  0.5556    0.1695    0.4727\n",
      "4       Overall  2251  1163  642     0.6593  0.7781    0.7138    0.7727\n",
      "\n",
      "================================================================================\n",
      "\n",
      "5. Fine-tuned GPT-4o-mini:\n",
      "    Entity Type    TP    FP   FN  Precision  Recall  F1 Score  F5 Score\n",
      "0  NAME_STUDENT  2507  1597  103     0.6109  0.9605    0.7468    0.9398\n",
      "1  URL_PERSONAL   199   206   13     0.4914  0.9387    0.6451    0.9069\n",
      "2         EMAIL    60    10    2     0.8571  0.9677    0.9091    0.9630\n",
      "3     PHONE_NUM     8     4    1     0.6667  0.8889    0.7619    0.8776\n",
      "4       Overall  2774  1817  119     0.6042  0.9589    0.7413    0.9377\n",
      "\n",
      "================================================================================\n",
      "\n",
      "6. Verifier Model I (Without CoT):\n",
      "    Entity Type    TP   FP   FN  Precision  Recall  F1 Score  F5 Score\n",
      "0  NAME_STUDENT  2098  278  512     0.8830  0.8038    0.8416    0.8066\n",
      "1  URL_PERSONAL   161    2   51     0.9877  0.7594    0.8587    0.7662\n",
      "2         EMAIL    60    8    2     0.8824  0.9677    0.9231    0.9642\n",
      "3     PHONE_NUM     2    1    7     0.6667  0.2222    0.3333    0.2281\n",
      "4       Overall  2321  289  572     0.8893  0.8023    0.8435    0.8053\n",
      "\n",
      "================================================================================\n",
      "\n",
      "7. Verifier Model II (With CoT):\n",
      "    Entity Type    TP   FP   FN  Precision  Recall  F1 Score  F5 Score\n",
      "0  NAME_STUDENT  2261  704  349     0.7626  0.8663    0.8111    0.8618\n",
      "1  URL_PERSONAL   173   74   39     0.7004  0.8160    0.7538    0.8109\n",
      "2         EMAIL    60    9    2     0.8696  0.9677    0.9160    0.9636\n",
      "3     PHONE_NUM     8    3    1     0.7273  0.8889    0.8000    0.8814\n",
      "4       Overall  2502  790  391     0.7600  0.8648    0.8091    0.8603\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the true data path and a dictionary mapping model names to their prediction files\n",
    "true_file_path = 'data/test_set.csv'\n",
    "models_to_evaluate = {\n",
    "    \"1. Presidio (en_core_web_lg)\": \"output/1_presidio_lg.csv\",\n",
    "    \"2. Presidio (en_core_web_trf)\": \"output/2_presidio_trf.csv\",\n",
    "    \"3. Azure AI Language\": \"output/3_azure.csv\",\n",
    "    \"4. Prompting GPT-4o-mini\": \"output/4_prompting.csv\",\n",
    "    \"5. Fine-tuned GPT-4o-mini\": \"output/5_finetuned.csv\",\n",
    "    \"6. Verifier Model I (Without CoT)\": \"output/6_verifier_I.csv\",\n",
    "    \"7. Verifier Model II (With CoT)\": \"output/7_verifier_II.csv\"\n",
    "}\n",
    "\n",
    "# Loop through the models, calculate metrics, and print the results\n",
    "for model_name, detected_file_path in models_to_evaluate.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    metrics_df = calculate_metrics(true_file_path, detected_file_path)\n",
    "    print(metrics_df)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
