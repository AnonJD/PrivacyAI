{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction: put in your model name and run cells sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_entities import *\n",
    "import pandas as pd\n",
    "model = '[Your GPT model here]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(text_list:List[str], identifier_list:List[str],store_directory:str)-> None: \n",
    "    system_prompt = \"You are an expert in labeling Personally Identifiable Information. Start your response rightaway without adding any prefix(such as Response:) and suffix\"\n",
    "    ins_prompt = '''Label the entity of the following text: use @@@,### to label student name\\n\n",
    "Ensure that the rest of the text remains unchanged, word for word.\n",
    "Maintain the original punctuation, quotation marks, spaces, and line breaks. \n",
    "If the text does not contain any PII, return it as is.\n",
    "For example, if the input is:\n",
    "student: Hi\\nstudent: Boria Lupu?\\nteacher: Hi Gurung Kool!\\nteacher: How are you today?\\nstudent: Fine\\n\n",
    "The output should be:\n",
    "student: Hi\\nstudent: @@@Boria Lupu###?\\nteacher: Hi @@@Gurung Kool###!\\nteacher: How are you today?\\nstudent: Fine\\n\n",
    "Another example:\n",
    "teacher: I presume 'agape' is Greek\\nstudent: I think so. It's a very rich language\\nstudent: I can ask our teacher Josue Rodrigo Nieto Hernandez. He knows Greek\\n\n",
    "The output should be:\n",
    "teacher: I presume 'agape' is Greek\\nstudent: I think so. It's a very rich language\\nstudent: I can ask our teacher @@@Josue Rodrigo Nieto Hernandez###. He knows Greek\\n\n",
    "Another example:\n",
    "student: Yesterday Okpan Ohabuike's mum had a surgery you know\\nteacher: Oh yes, of course! Is she ok?\n",
    "The output should be exactly the same as input:\n",
    "student: Yesterday @@@Okpan Ohabuike###'s mum had a surgery you know\\nteacher: Oh yes, of course! Is she ok?\n",
    "Please repeat this process with the following file:\\n\n",
    "'''\n",
    "    # model=\"gpt-4o-mini-2024-07-18\"\n",
    "    model= model\n",
    "    try:\n",
    "        os.makedirs(store_directory, exist_ok=True)\n",
    "        print(f\"Directory '{store_directory}' is ready.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory '{store_directory}': {e}\")\n",
    "    res = []\n",
    "    for i in range(len(text_list)): \n",
    "        request = {\"custom_id\": f'{identifier_list[i]}', \"method\": \"POST\", \n",
    "                  \"url\": \"/v1/chat/completions\", \n",
    "                  \"body\": {\"model\": model, \"messages\": [{\"role\": \"system\", \"content\": system_prompt},{\"role\": \"user\", \"content\": ins_prompt + text_list[i]}],\n",
    "                  \"temperature\":0}}\n",
    "        res.append(request)\n",
    "    \n",
    "    store_directory += '/batch_request.jsonl'\n",
    "    with open(store_directory, 'w',encoding='utf-8') as f:\n",
    "        for entry in res:\n",
    "            json_line = json.dumps(entry, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/new_texts.txt', 'r') as f:\n",
    "    input_texts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the directory to store the temporary files\n",
    "gpt_storage_directory = \"gpt_output_ft+pt\"\n",
    "identifier_list = [f'doc{i}' for i in range(len(input_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make batch file for submitting finetune job \n",
    "make_batch(input_texts, identifier_list,gpt_storage_directory)\n",
    "\n",
    "#submit finetune job \n",
    "submit_finetune_job(gpt_storage_directory)\n",
    "\n",
    "#print the progress\n",
    "check_progress(gpt_storage_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_progress(gpt_storage_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check progress, if completed, then retrieve the file_id\n",
    "out_id = check_progress(gpt_storage_directory)\n",
    "if out_id != None: \n",
    "    #if comleted, retrieve the result and store the result in a file.\n",
    "    file_response = client.files.content(out_id)\n",
    "    save_file_response_as_jsonl(file_response, gpt_storage_directory)  \n",
    "    data_list = read_jsonl_file(gpt_storage_directory)\n",
    "    data = resort_data(data_list,identifier_list)\n",
    "    batch_extract_parse_store(data,identifier_list,gpt_storage_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_entities = extract_entities(r'gpt_output_prompting/gpt_result.json')\n",
    "df = pd.DataFrame(detected_entities, columns=['file_idx', 'entity_text', 'positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'results/TSCC_detected_prompting.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
