{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load input texts and ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_texts or new_texts_FN\n",
    "with open('new_texts.txt', 'r') as f:\n",
    "    input_texts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Run Presidio on input text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_trf\"}],\n",
    "}\n",
    "\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine = provider.create_engine()\n",
    "\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine = nlp_engine,\n",
    "    supported_languages=[\"en\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(input_text):\n",
    "    results_analyzed = analyzer.analyze(text=input_text,\n",
    "                                        entities=['PERSON'],\n",
    "                                        language=\"en\",\n",
    "                                        return_decision_process=True)\n",
    "    return results_analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_entities = []\n",
    "for file_idx, text in enumerate(input_texts):\n",
    "    print(f\"Processing File {file_idx}\")\n",
    "    results_analyzed = analyze_text(text)\n",
    "\n",
    "    for res in results_analyzed:\n",
    "        s, e = res.start, res.end\n",
    "        entity_text = input_texts[file_idx][s:e]\n",
    "        detected_entities.append([file_idx, entity_text, (s, e)])\n",
    "\n",
    "# Total number of files: 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(detected_entities, columns=['file_idx', 'entity_text', 'positions'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'results/TSCC_detected_pre_FN.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
